{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Problem.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "i8YRlFLjD7rr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this notebook I present my approach to the given Computer Vision Problem. I implemented this problem in Pytorch. "
      ]
    },
    {
      "metadata": {
        "id": "RsvwnDXvE2J7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing Important Libraries**"
      ]
    },
    {
      "metadata": {
        "id": "PbHmOv8CD0Ny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle # we use pickle to extract the data from the .pkl files given to us\n",
        "import numpy as np # for processing tensors and numpy arrays\n",
        "import pandas as pd \n",
        "from matplotlib import image # for image processing\n",
        "import matplotlib.pyplot as plt # for plotting graphs\n",
        "import torch # for using torch\n",
        "from torch import nn # for defining layers of the neural network\n",
        "import torch.nn.functional as F # to apply different activations on the layers\n",
        "from torchvision import datasets,models,transforms # for creating dataloaders and for data augmentation and normalization\n",
        "import shutil # for file handling\n",
        "from PIL import Image # for image processing\n",
        "from sklearn.model_selection import train_test_split # for creating a training set and a validation set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8E7xvgzhI-Tq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "2xVYS0iFGPV8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reading Files Into The memory**\n",
        "\n",
        "We use python's pickle library to load data from pickle files\n",
        "\n",
        "\n",
        "*   train_data contains list of training images.\n",
        "*   train_labels contains the corresponding class each image belongs to.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "40nHUiNTD0N3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('train_image.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "with open('train_label.pkl', 'rb') as g:\n",
        "    train_labels = pickle.load(g)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hzHCn4KIT1Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reshaping the data to view the images**\n",
        "Since the pixel values of all the images in training, validation and test set are stored as one vector, we will need to reshape the vector to form a image out of it and to be able to run a CNN model.\n",
        "\n",
        "Since number of pixel for a training set were 784, an obvious guess was to try to reshape it into (28x28), because 28x28=784. We then checked whether the reshaped image make some sense or not by plotting it using matplotlib library of python."
      ]
    },
    {
      "metadata": {
        "id": "UBqavrvlD0N6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = np.array(data)# converting the data to an array\n",
        "train_data = np.reshape(image,(8000,28,28)) # reshaping the data so that we can view the images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULFnj3D3D0N_",
        "colab_type": "code",
        "colab": {},
        "outputId": "62008b21-d860-4a08-8e1d-b7d5b822c412"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(train_data[0],cmap='gray')# viewing the images in Grayscale"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5c0f58c978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ojb1K5K2D0OF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = np.reshape(train_data,(8000,28,28,1)) # Here we again reshape the data(adding a third dimension for number of channels) so that we can insert it into the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHzztYFXD0OL",
        "colab_type": "code",
        "colab": {},
        "outputId": "16c5cbd1-b9d0-451f-ef00-af650dbbd160"
      },
      "cell_type": "code",
      "source": [
        "classes = set(train_labels) # to view the unique classes the images belong to.\n",
        "classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 2, 3, 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "UwGw2MYoKr-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Dividing the data into train and dev(validation) sets using train_test_split.**\n",
        "The data is divided into 80% training images and 20% validation images"
      ]
    },
    {
      "metadata": {
        "id": "azTQ5j0dD0OY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UN7jdmwGJuem",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Organising the images into folders 'train' and 'valid'  each folder containing subfolders corresponding to the labels for the images**  "
      ]
    },
    {
      "metadata": {
        "id": "IBfwEtn0D0OQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir('train')# making a train directory\n",
        "\n",
        "for i in classes:\n",
        "    os.mkdir('train/'+str(i)) # making subfolders for organising images according to their classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_PLp4MJrD0OU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.mkdir('valid')# making a valid directory\n",
        "\n",
        "for i in classes:\n",
        "    os.mkdir('valid/'+str(i))# making subfolders for organising images according to their classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zbseDnMALYag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each numpy array is converted into an image and is then saved into its respective folder corresponding to its labels.\n",
        "This is done for both training and validation sets\n",
        "\n",
        "\n",
        "\n",
        "*   Structure of the data\n",
        "\n",
        "\n",
        "     train(directory)\n",
        "        |0(subdirectory)\n",
        "        |2(subdirectory)\n",
        "        |3(subdirectory)\n",
        "        |6(subdirectory)\n"
      ]
    },
    {
      "metadata": {
        "id": "pWq835OfMhmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each subdirectory contains images with labels same as the subdirectory name"
      ]
    },
    {
      "metadata": {
        "id": "vh3J3TXfD0Ob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(X_train.shape[0]):\n",
        "    img = Image.fromarray(np.array(X_train[i].reshape(28,28)).astype('uint8'))# image converted from numpy array into image\n",
        "    img.save('train/'+str(y_train[i])+'/'+str(i)+'.jpg')# image is saved into the respective location\n",
        "\n",
        "for i in range(X_valid.shape[0]):\n",
        "    img = Image.fromarray(np.array(X_valid[i].reshape(28,28)).astype('uint8'))# image converted from numpy array into image\n",
        "    img.save('valid/'+str(y_valid[i])+'/'+str(i)+'.jpg')# image is saved into the respective location"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YgLslkA5OdN0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating DataLoaders for Training and Validation sets**\n",
        "Here Data Augmentation has not been used. I tried running the model using data augmentations on the images such as Horizontal flipping , rotation. But the model gave the same results with or without data augmentation therefore data augmentation has not been used as it increases the training time.\n",
        "\n",
        "**The 1 channel grayscale images have been converted to 3 channels images where each channel is made of the same GrayScale image.**\n",
        "This approach allows the model to learn more from the same image."
      ]
    },
    {
      "metadata": {
        "id": "4sqlhj4lD0Og",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trans = transforms.Compose([transforms.Grayscale(3),transforms.ToTensor()])# to transform the images from 1-channel to 3-channel and convert it into a pytorch tensor\n",
        "trainset = datasets.ImageFolder('train/',transform=trans)# to load data from the train folder\n",
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size=100,shuffle=True)\n",
        "\n",
        "validset = datasets.ImageFolder('valid/',transform=trans)# to load data from the train folder\n",
        "validloader = torch.utils.data.DataLoader(validset,batch_size=100,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yn31TzjySZJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Now we define the Train Function which prints the training_loss, validation_loss, validation accuracy and training accuracy calculates forward and back propagation** \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-Qh22d68D0Oi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model,trainloader,validloader,opt,criterion,sched,epochs=10):\n",
        "    model.cuda()# passing the model weights to the gpu\n",
        "    for e in range(epochs):# looping over each epoch\n",
        "        train_loss = 0\n",
        "        train_accuracy = 0\n",
        "        model.train()# model is toggled to training mode this enables the model's dropout layers.\n",
        "        for images,labels in trainloader:# loading each batch of the training data\n",
        "            images = images.cuda()# passing the data to gpu for faster calculations\n",
        "            labels = labels.cuda()# passing the data to gpu\n",
        "            log_probs = model(images)# paasing the data through the model\n",
        "            loss = criterion(log_probs,labels)# calculating the loss. The loss function is categorical CrossEntropyLoss function\n",
        "            ps = torch.exp(log_probs)# for calculating the accuracy we calculate the probability of each class\n",
        "            _,top_class = ps.topk(1,dim=1)# then the class with the highest probability is extracted\n",
        "            equals = top_class == labels.view(top_class.shape)# number of correct predictions is calculated\n",
        "            train_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "            opt.zero_grad()\n",
        "            loss.backward() # Back propagation\n",
        "            opt.step() # optimizer step\n",
        "            train_loss += loss.item() \n",
        "        else:\n",
        "            valid_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()# for disbling the dropouts of the model during inferencing\n",
        "            with torch.no_grad():# for disabling the calculation of the gradient for validation set\n",
        "                for images,labels in validloader:# loading each batch of the validation data\n",
        "                    images = images\n",
        "                    images = images.cuda()# passing the data to gpu for faster calculations\n",
        "                    labels = labels.cuda()# passing the data to gpu for faster calculations\n",
        "                    logps = model(images)# paasing the data through the model\n",
        "                    valid_loss += criterion(logps,labels)# calculating the validation loss. The loss function is categorical CrossEntropyLoss function\n",
        "                    ps = torch.exp(logps)# for calculating the accuracy we calculate the probability of each class\n",
        "                    _,top_class = ps.topk(1,dim=1)# then the class with the highest probability is extracted\n",
        "                    equals = top_class == labels.view(top_class.shape)# number of correct predictions is calculated\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "            sched.step(valid_loss/len(validloader))\n",
        "            print('epoch :{}/{} '.format(e+1,epochs),\n",
        "                'train_loss :{:.3f} '.format(train_loss/len(trainloader)),# printing average training loss\n",
        "                'test_loss :{:.3f} '.format(valid_loss/len(validloader)),# printing average validation loss\n",
        "                'test accuracy: {:.3f} '.format(accuracy/len(validloader)),# printing average validation accuracy\n",
        "                'train accuracy: {:.3f}'.format(train_accuracy/len(trainloader)))# printing average train accuracy\n",
        "            if e%5 == 0:\n",
        "                torch.save({'model_state':model.state_dict(),'optimizer_state':optimizer.state_dict()},'models_new_3c_dr0.25_conv_dropout_epoch7_wn')\n",
        "                #saving model weights after every 5 epochs as check points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KyGDXCA1ZmFY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Building The Convolutional Neural Network**\n",
        "\n",
        "Here I have used convolutional neural network because it will work best on the image data.\n",
        "It will perform better than Artificial Neural Networks. Transfer Learning from the imagenet dataset has not been used because it does not give good results on the dataset.\n",
        "\n",
        "In this model convolutional , Batch Normalizations , Max pooling  and Dropout layers have been used . ReLU function has been used as the activation after all layers except the last fully connected layers. Dropout layers help reducing variance of the model. Max Pooling highlight the main features of an image from  previous layers. Batch Normalization helps in speeding up calculations because it reduces the mean to 0 and standard deviation to 1. Then the output from all the convolutional layers is flattened and is passed into the fully connected layers which gives the final probabilities of each class for each input using the softmax activation function "
      ]
    },
    {
      "metadata": {
        "id": "aOSKLuyQD0Ol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class my_model(nn.Module):\n",
        "    def __init__(self,batch_size):\n",
        "        super().__init__()\n",
        "        self.bs = batch_size\n",
        "        self.convol = nn.Sequential(\n",
        "        nn.Conv2d(3,32,kernel_size=3,stride=1,padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Dropout(0.2),\n",
        "            \n",
        "        nn.Conv2d(32,64,kernel_size=3,stride=1,padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Dropout(0.1),\n",
        "        \n",
        "        nn.Conv2d(64,128,kernel_size=3,stride=1,padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Dropout(0.1),\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "                                nn.Linear(3200,2048),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(0.2),\n",
        "            \n",
        "                                nn.Linear(2048,1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(0.1),\n",
        "            \n",
        "                                nn.Linear(1024,512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(0.1),\n",
        "            \n",
        "                                nn.Linear(512,256),\n",
        "                                nn.ReLU(),\n",
        "            \n",
        "                                nn.Linear(256,4))\n",
        "        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.convol(x)\n",
        "        x = x.view(self.bs,-1)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x,dim=1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "28gnCwCxdE9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Before Training the model** \n",
        "\n",
        "We use ReduceLROnPlateau for decreasing the learning rate if the validation loss does not decrease for a particular number of epochs"
      ]
    },
    {
      "metadata": {
        "id": "XItV0u2lD0Oo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = my_model(batch_size = 100)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IBgYPoEOdqSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training the model**\n",
        "\n",
        "Finally we train the model. Each epoch prints the training loss , the validation loss, the validation accuracy and the training accuracy. The loss used is Categorical cross entropy loss which is divided into two parts log_softmax and negative log likelihood loss."
      ]
    },
    {
      "metadata": {
        "id": "EBwCTNR_D0Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(model=model,trainloader=trainloader,validloader=validloader,opt=optimizer,criterion=criterion,sched=scheduler,epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJOISAQvea5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inferencing"
      ]
    },
    {
      "metadata": {
        "id": "huIlla88eem_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**loading the best model**"
      ]
    },
    {
      "metadata": {
        "id": "4QdT6fyuD0Oy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "st_dict = torch.load('models_new_3c_dr0.25_conv_dropout_epoch7_wn',map_location='cpu')# dictionary containing weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eHKdpJjD0O3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = my_model(batch_size = 1)# initializing model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_owtcvGD0O7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(st_dict['model_state']) # loading the weights into the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aUkZ3_FID0PC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('test_image.pkl', 'rb') as c: # unpickling the test data\n",
        "    data = pickle.load(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fsqSQmpmD0PF",
        "colab_type": "code",
        "colab": {},
        "outputId": "5c4644bb-b2be-4326-933e-8eca3b18c53a"
      },
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "tkR9wXvaD0PI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imgs = np.array(data)# converting the data to numpy array\n",
        "test_imgs = np.reshape(imgs,(2000,28,28,1))# reshaping the data so that it can be loaded into the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tOF9F1dpD0PK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trans = transforms.Compose([transforms.Grayscale(3),transforms.ToTensor()])# transforming 1-channel images to 3-channel images and converting the images into pytorch tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "thuTNx7TD0PN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_mp = {'tensor(0)':0,'tensor(1)':2,'tensor(2)':3,'tensor(3)':6} #mapping labels with the respective classes to which the images belong to"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "60XkdliTfmcw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**predicting the labels for the images in the test set**"
      ]
    },
    {
      "metadata": {
        "id": "lfG795DaD0PQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_label = []\n",
        "for i in range(test_imgs.shape[0]):\n",
        "    img = Image.fromarray(np.array(test_imgs[i].reshape(28,28)).astype('uint8'))# reshaping the image for inputting into the model\n",
        "    img = trans(img)# transforming the image\n",
        "    img = torch.unsqueeze(img,dim=0)# adding an extra dimension for number of channels into the images\n",
        "    log_ps = model(img)\n",
        "    ps = torch.exp(log_ps)\n",
        "    _,top_cl = ps.topk(1,dim=1)# taking out the index of the class with the maximum probability\n",
        "    test_label.append(top_cl[0][0])#appending the labels into a list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cw-VV8mzD0PT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "orig_labels = [label_mp[str(i)] for i in test_label]# converting the labels to original classes such as '0'->0,'1'->2,'2'->3,'3'->6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHYjrVj4gdst",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**preparing the submission file**"
      ]
    },
    {
      "metadata": {
        "id": "9HrzUcSqD0PV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_rows = []\n",
        "for i in range(len(orig_labels)):\n",
        "    l = []\n",
        "    l.append(i)\n",
        "    l.append(orig_labels[i])\n",
        "    all_rows.append(l)\n",
        "\n",
        "Preds = pd.DataFrame(all_rows,columns=['image_index','class'])\n",
        "Preds.to_csv('Ritik_Mahajan.csv',index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0Lle35KD0PY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
