{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,models,transforms\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_image.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(data)\n",
    "image = np.reshape(image,(8000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5c0f58c978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.reshape(image,(8000,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_label.pkl', 'rb') as g:\n",
    "    labels = pickle.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 3, 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = set(labels)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('train')\n",
    "\n",
    "for i in classes:\n",
    "    os.mkdir('train/'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('valid')\n",
    "\n",
    "for i in classes:\n",
    "    os.mkdir('valid/'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(image_data, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    img = Image.fromarray(np.array(X_train[i].reshape(28,28)).astype('uint8'))\n",
    "    img.save('train/'+str(y_train[i])+'/'+str(i)+'.jpg')\n",
    "\n",
    "for i in range(X_valid.shape[0]):\n",
    "    img = Image.fromarray(np.array(X_valid[i].reshape(28,28)).astype('uint8'))\n",
    "    img.save('valid/'+str(y_valid[i])+'/'+str(i)+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Grayscale(3),transforms.ToTensor(),transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "trainset = datasets.ImageFolder('train/',transform=trans)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=100,shuffle=True)\n",
    "\n",
    "validset = datasets.ImageFolder('valid/',transform=trans)\n",
    "validloader = torch.utils.data.DataLoader(validset,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,trainloader,validloader,opt,criterion,sched,epochs=10):\n",
    "    model.cuda()\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        model.train()\n",
    "        for images,labels in trainloader:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            log_probs = model(images)\n",
    "            loss = criterion(log_probs,labels)\n",
    "            ps = torch.exp(log_probs)\n",
    "            _,top_class = ps.topk(1,dim=1)\n",
    "            equals = top_class == labels.view(top_class.shape)\n",
    "            train_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        else:\n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for images,labels in validloader:\n",
    "                    images = images\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    logps = model(images)\n",
    "                    valid_loss += criterion(logps,labels)\n",
    "                    ps = torch.exp(logps)\n",
    "                    _,top_class = ps.topk(1,dim=1)\n",
    "                    equals = top_class == labels.view(top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            sched.step(valid_loss/len(validloader))\n",
    "            print('epoch :{}/{} '.format(e+1,epochs),\n",
    "                'train_loss :{:.3f} '.format(train_loss/len(trainloader)),\n",
    "                'test_loss :{:.3f} '.format(valid_loss/len(validloader)),\n",
    "                'test accuracy: {:.3f} '.format(accuracy/len(validloader)),\n",
    "                'train accuracy: {:.3f}'.format(train_accuracy/len(trainloader)))\n",
    "            if e%5 == 0:\n",
    "                torch.save({'model_state':model.state_dict(),'optimizer_state':optimizer.state_dict()},'models_new_1c_conv_dropout_epoch40')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model(nn.Module):\n",
    "    def __init__(self,batch_size):\n",
    "        super().__init__()\n",
    "        self.bs = batch_size\n",
    "        self.convol = nn.Sequential(\n",
    "        nn.Conv2d(3,32,kernel_size=3,stride=1,padding=2),\n",
    "        \n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        \n",
    "        nn.Conv2d(32,64,kernel_size=3,stride=1,padding=2),\n",
    "        \n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        \n",
    "        nn.Conv2d(64,128,kernel_size=3,stride=1,padding=2),\n",
    "        \n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                                nn.Linear(3200,2048),\n",
    "                                nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                nn.Linear(2048,1024),\n",
    "                                nn.ReLU(),\n",
    "                                 nn.Dropout(0.1),\n",
    "                                nn.Linear(1024,512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512,256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(256,4))\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.convol(x)\n",
    "        x = x.view(self.bs,-1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        #print(x.shape)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(batch_size = 100)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model,trainloader=trainloader,validloader=validloader,opt=optimizer,criterion=criterion,sched=scheduler,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state':model.state_dict(),'optimizer_state':optimizer.state_dict()},'models_new_1c_conv_dropout_epoch40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dict = torch.load('models_new_3cconv_dropout_epoch40',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(st_dict['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_image.pkl', 'rb') as c:\n",
    "    data = pickle.load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.array(data)\n",
    "test_imgs = np.reshape(imgs,(2000,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.Grayscale(3),transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mp = {'tensor(0)':0,'tensor(1)':2,'tensor(2)':3,'tensor(3)':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_label = []\n",
    "for i in range(test_imgs.shape[0]):\n",
    "    img = Image.fromarray(np.array(test_imgs[i].reshape(28,28)).astype('uint8'))\n",
    "    img = trans(img)\n",
    "    img = torch.unsqueeze(img,dim=0)\n",
    "    log_ps = model(img)\n",
    "    ps = torch.exp(log_ps)\n",
    "    _,top_cl = ps.topk(1,dim=1)\n",
    "    test_label.append(top_cl[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_labels = [label_mp[str(i)] for i in test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "for i in range(len(orig_labels)):\n",
    "    l = []\n",
    "    l.append(i)\n",
    "    l.append(orig_labels[i])\n",
    "    all_rows.append(l)\n",
    "\n",
    "Preds = pd.DataFrame(all_rows,columns=['image_index','class'])\n",
    "Preds.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
